
# âš¡ Neural Machine Translation Insights & Evaluation Platform

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-%3E%3D3.8-green.svg)
![Status](https://img.shields.io/badge/build-stable-success)

A lightweight, developer-friendly platform for benchmarking, debugging, and interpreting Neural Machine Translation (NMT) models.  
Built for engineers who want **clarity in outputs**, **control over metrics**, and **speed in experimentation**.

---

## ğŸ§  What This Platform Delivers

- âš™ï¸ **Side-by-side comparison** of multiple NMT models  
- ğŸ“Š **BLEU** + **Entity-Focused Correctness (EFC)** for semantic evaluation  
- ğŸ–¥ï¸ **Streamlit-powered UI** for rapid iteration & testing  
- ğŸ” Visual debugging tools to understand model failures  
- ğŸ”§ Modular codebase that fits naturally into research or production R&D

This repo is crafted to give you **real insights**, not just output strings.

---

## ğŸš€ Core Features

### ğŸ— Model Variants
- Transformer (Baseline)
- Entity-Aware Contrastive Fine-Tuning (EACT)
- Retrieval-Guided Constrained Lattice Decoding (RG-CLD)

### ğŸ“ Metrics
- BLEU Score
- Entity-Focused Correctness (EFC)  
  *(Because token-level metrics alone donâ€™t tell the whole story.)*

### ğŸ–¥ Developer UI
- Real-time inference  
- Multi-model output comparison  
- Entity-level alignment visualizations  
- Metric plots & debugging hooks  

---

## âš¡ Quickstart

### Clone
```bash
git clone https://github.com/caffe-cappuccino/Neural_Machine_Translation_Insights_Evaluation_Platform.git
cd Neural_Machine_Translation_Insights_Evaluation_Platform
````

### Install

```bash
pip install -r requirements.txt
```

### Run

```bash
streamlit run app.py
```

---

## ğŸ“‚ Directory Layout

```
.
â”œâ”€â”€ models/            # Checkpoints, retrieval files, model weights
â”œâ”€â”€ utils/             # Metrics, preprocessors, helpers
â”œâ”€â”€ app.py             # Streamlit frontend
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

Everything is modular. Everything is hackable.

---

## ğŸ§© Workflow

1. Input text or upload a dataset
2. Generate translations across selected models
3. Compare metrics (BLEU + EFC)
4. Explore visual breakdowns
5. Debug entity mismatches, drift, hallucinations
6. Iterate fast. Deploy faster.

---

## ğŸ”§ Extend Like a Pro

Adding a new model?

```bash
# drop model files â†’ /models
# implement wrapper â†’ /utils
# register in UI â†’ app.py
```

Adding a new metric?

```bash
# write metric function â†’ /utils
# add to evaluation pipeline
# visualize in Streamlit
```

This repo is designed with **clean separations**, **functional modules**, and **plug-and-play architecture**.

---

## ğŸ“– References

* *Attention Is All You Need*, Vaswani et al. (2017)
* Research on Entity-Aware MT Fine-Tuning
* Retrieval-guided decoding approaches

---

## ğŸ¤ Contributions

Pull requests, issue reports, and model extensions are always welcome.
If you break something, improve something, or optimize something â€” ship it ğŸš€

---

## ğŸ“ License

MIT License â€” free to build, break, improve, and ship.

---

**Built for engineers who care about clarity, metrics, and control.**
Happy hacking âš¡

